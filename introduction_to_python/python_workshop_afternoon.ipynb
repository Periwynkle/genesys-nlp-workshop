{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Python for Digital Text Analysis (Part I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session will provide an overview of the Python Natural Language Toolkit (NLTK) library (http://www.nltk.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step I: Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step II: Read in comment files\n",
    "Let's choose four popular songs, one from each of the groups, and import their comment text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First need to find the comment-only files.\n",
    "bts_filepath = '../data/kpop_videos_comments/bts/GZjt_sA2eso.txt' # Save Me\n",
    "exo_filepath = '../data/kpop_videos_comments/exo/yWfsla_Uh80.txt' # Call Me Baby\n",
    "twice_filepath = '../data/kpop_videos_comments/twice/EpMwiqW8k8o.txt' # 'Signal' Dance Video\n",
    "blackpink_filepath = '../data/kpop_videos_comments/blackpink/bwmSjveL3Lc.txt' # Boombayah\n",
    "\n",
    "# Read in the text from the files as strings, using UTF 8 encoding to recognise emoji.\n",
    "with open(bts_filepath, encoding=\"utf-8\") as text:\n",
    "    bts = text.read()\n",
    "with open(exo_filepath, encoding=\"utf-8\") as text:\n",
    "    exo = text.read()\n",
    "with open(twice_filepath, encoding=\"utf-8\") as text:\n",
    "    twice = text.read()  \n",
    "with open(blackpink_filepath, encoding=\"utf-8\") as text:\n",
    "    blackpink = text.read()\n",
    "    \n",
    "print(bts[:300]) # Print first 300 characters\n",
    "#print(bts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step III: Tokenise the comments\n",
    "\n",
    "NLTK word tokenizer (ignores non-alpha characters) vs. tweet tokenizer (represents hashtags, @mentions, and emoji / doesn't strip them away): http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "We will use the tweet tokenizer, as it is more applicable to YouTube comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in TweetTokenizer().tokenize(bts[:300]):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step IV: Calculate type / token counts\n",
    "From this, we can also calculate lexical diversity with a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length of a text (number of words and punctuation symbols).\n",
    "len(bts)\n",
    "\n",
    "#All vocabulary items in a text.\n",
    "#set(bts) #Takes a very long time!\n",
    "\n",
    "#Sort the vocabulary items in alphabetical order (punctuation comes first, then capitalised words).\n",
    "#sorted(set(bts)) #Freezes the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of unique vocabulary items (includes punctuation symbols).\n",
    "len(set(bts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexical richness (number of distinct words/number of total words).\n",
    "#How many times on average is each word used? Divide 100 by the result to get this.\n",
    "\n",
    "def lexical_diversity(comments):\n",
    "    return len(set(comments))/len(comments)\n",
    "\n",
    "lexical_diversity(bts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How often a specific word appears in a text.\n",
    "print(bts.count(\"jimin\"))\n",
    "\n",
    "#Percentage of the text taken up by a specific word.\n",
    "def word_percentage(wordcount, wordtotal):\n",
    "    return 100 * wordcount / wordtotal\n",
    "\n",
    "word_percentage(bts.count(\"jimin\"), len(bts))\n",
    "\n",
    "#Can look at other band member names and do a simple frequency bar chart to compare their 'popularity' in the comments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency distribution of vocabulary items.\n",
    "fdist1 = FreqDist(bts)\n",
    "print(bts)\n",
    "#An \"outcome\" is a word.\n",
    "\n",
    "#50 most frequent words.\n",
    "fdist1.most_common(50)\n",
    "\n",
    "#Frequency of a specific word.\n",
    "fdist1[\"jimin\"]\n",
    "\n",
    "#Cumulative frequency plot. If cumulative=True is not specified, individual frequencies are plotted.\n",
    "fdist1.plot(50, cumulative=True)\n",
    "\n",
    "#Words that only occur once.\n",
    "#fdist1.hapaxes()\n",
    "\n",
    "#Examine all of the long words (more than 7 letters) in a text.\n",
    "Vocab = set(bts)\n",
    "long_words = [word for word in Vocab if len(word)>7]\n",
    "sorted(long_words)\n",
    "\n",
    "#Examine only the long words that occur more than 7 times.\n",
    "fdist5 = FreqDist(bts)\n",
    "long_frequent_words = [word for word in Vocab if len(word)>7 and fdist5[word]>7]\n",
    "sorted(long_frequent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step V: POS tagging\n",
    "\n",
    "Compute and visualise frequencies of most popular (proper) nouns, adjectives, verbs. Also frequencies of most popuar words overall...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag(bts)\n",
    "nltk.pos_tag(bts, tagset=\"universal\")\n",
    "\n",
    "##Universal Part-of-Speech Tagset\n",
    "##Tag\tMeaning\t                English Examples\n",
    "##ADJ\tadjective\t        new, good, high, special, big, local\n",
    "##ADP\tadposition\t        on, of, at, with, by, into, under\n",
    "##ADV\tadverb\t                really, already, still, early, now\n",
    "##CONJ\tconjunction\t        and, or, but, if, while, although\n",
    "##DET\tdeterminer, article\tthe, a, some, most, every, no, which\n",
    "##NOUN\tnoun\t                year, home, costs, time, Africa\n",
    "##NUM\tnumeral\t                twenty-four, fourth, 1991, 14:24\n",
    "##PRT\tparticle\t        at, on, out, over per, that, up, with\n",
    "##PRON\tpronoun\t                he, their, her, its, my, I, us\n",
    "##VERB\tverb\t                is, say, told, given, playing, would\n",
    "##.\tpunctuation marks\t. , ; !\n",
    "##X\tother\t                ersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "# Adapt the below code to the Kpop dataset!\n",
    "\n",
    "#TAGGED CORPORA\n",
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories=\"news\", tagset=\"universal\")\n",
    "tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
    "tag_fd.most_common()\n",
    "tag_fd.plot(cumulative=True)\n",
    "\n",
    "#Which parts of speech occur before a noun?\n",
    "word_tag_pairs = nltk.bigrams(brown_news_tagged) #Bigrams consist of word-tag pairs.\n",
    "noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == \"NOUN\"]\n",
    "fdist_noun_preceders = nltk.FreqDist(noun_preceders)\n",
    "fdist_noun_preceders.most_common() #Displays tags and frequencies.\n",
    "[tag for (tag, _) in fdist_noun_preceders.most_common()] #Just displays tags.\n",
    "\n",
    "#What are the most common verbs in the Wall Street Journal corpus?\n",
    "wsj = nltk.corpus.treebank.tagged_words(tagset=\"universal\")\n",
    "word_tag_fd = nltk.FreqDist(wsj)\n",
    "word_tag_fd.most_common(50)\n",
    "#[wordtag[0] for (wordtag, _) in word_tag_fd.most_common() if wordtag[1] == \"VERB\"] #Sort verbs by frequency.\n",
    "\n",
    "#Frequency-ordered list of POS tags given a word. Word is treated as a condition and its tag as an event.\n",
    "cfd1 = nltk.ConditionalFreqDist(wsj)\n",
    "cfd1[\"yield\"].most_common()\n",
    "\n",
    "#Reverse the order of the pairs to see likely words for a given POS tag.\n",
    "wsj2 = nltk.corpus.treebank.tagged_words()\n",
    "cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in wsj)\n",
    "list(cfd2[\"VBN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step VI: Bigrams & Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract a list of word pairs from a text.\n",
    "from nltk import bigrams\n",
    "list(bigrams([\"more\", \"is\", \"said\", \"than\", \"done\"]))\n",
    "\n",
    "#Collocations: bigrams that occur more often than we would expect based on the frequency of the individual words.\n",
    "bts.collocations()\n",
    "\n",
    "#Distribution of word lengths in a text.\n",
    "word_lengths = [len(word) for word in bts]\n",
    "fdist1wordlength = FreqDist(word_lengths)\n",
    "print(fdist1wordlength)\n",
    "\n",
    "#Most common word lengths.\n",
    "fdist1wordlength.most_common()\n",
    "\n",
    "#Most frequent word length.\n",
    "fdist1wordlength.max()\n",
    "\n",
    "#How many words of length 3 appear in the text.\n",
    "fdist1wordlength[3]\n",
    "\n",
    "#What proportion of all word lengths are words of length 3?\n",
    "fdist1wordlength.freq(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open-Ended Exercises/Questions\n",
    "1. What are the most common 3-grams, 4-grams..?\n",
    "2. Compare most frequent words (and types of words) in each of the four video comment datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
