{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Python for Digital Text Analysis (Part II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session will provide an overview of the Python Natural Language Toolkit (NLTK) library (http://www.nltk.org), which is an excellent platform for examining linguistic data. It has built-in corpora and text processing libraries for everything from tokenisation to semantic reasoning. NLTK is also favoured by teachers of computational linguistics.\n",
    "\n",
    "We will apply some basic NLTK functionalities to a few YouTube comment files in our Kpop dataset, and examine them individually as well as comparatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step I: Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "from nltk import bigrams\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step II: Read in comment files\n",
    "Let's choose four popular songs, one from each of the Kpop groups, and import their comment text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to save the paths of the comment-only files we are using.\n",
    "bts_filepath = '../data/kpop_videos_comments/bts/GZjt_sA2eso.txt' # Save Me\n",
    "exo_filepath = '../data/kpop_videos_comments/exo/yWfsla_Uh80.txt' # Call Me Baby\n",
    "twice_filepath = '../data/kpop_videos_comments/twice/EpMwiqW8k8o.txt' # 'Signal' dance video\n",
    "blackpink_filepath = '../data/kpop_videos_comments/blackpink/bwmSjveL3Lc.txt' # Boombayah\n",
    "\n",
    "# Next, let's read in the text from the files as strings, using UTF 8 encoding to recognise emoji.\n",
    "with open(bts_filepath, encoding=\"utf-8\") as text:\n",
    "    bts = text.read()\n",
    "with open(exo_filepath, encoding=\"utf-8\") as text:\n",
    "    exo = text.read()\n",
    "with open(twice_filepath, encoding=\"utf-8\") as text:\n",
    "    twice = text.read()  \n",
    "with open(blackpink_filepath, encoding=\"utf-8\") as text:\n",
    "    blackpink = text.read()\n",
    "\n",
    "print(bts[:300]) # Print first 300 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove CommentTextDisplay from all files by stripping off the first 19 characters (part of metadata).\n",
    "bts = bts[19:]\n",
    "exo = exo[19:]\n",
    "twice = twice[19:]\n",
    "blackpink = blackpink[19:]\n",
    "print(bts[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step III: Tokenise the comments\n",
    "\n",
    "Now that we've read in the comment files as strings, let's tokenize them so that we can analyse their words in various ways. This will transform the strings into lists of 'words'.\n",
    "\n",
    "NLTK's default word tokenizer ignores non-alpha characters (e.g., hashtags and emoji). We will use its tweet tokenizer, which recognises such characters and does not strip them away: http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ignore cases, so that 'BTS' and 'bts' for example are treated as the same type.\n",
    "bts_tokenized = TweetTokenizer().tokenize(bts.lower())\n",
    "exo_tokenized = TweetTokenizer().tokenize(exo.lower())\n",
    "twice_tokenized = TweetTokenizer().tokenize(twice.lower())\n",
    "blackpink_tokenized = TweetTokenizer().tokenize(blackpink.lower())\n",
    "print(bts_tokenized[:50]) # The first 50 'words'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step IV: Word-level calculations\n",
    "\n",
    "After tokenizing our comment files, we can count their 'words' in a variety of different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of a text (token count). Note that punctuation symbols and emoji are also counted as tokens.\n",
    "print(len(bts_tokenized))\n",
    "print(len(exo_tokenized))\n",
    "print(len(twice_tokenized))\n",
    "print(len(blackpink_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique vocabulary items (type count). Sets in Python contain unique objects.\n",
    "print(len(set(bts_tokenized)))\n",
    "print(len(set(exo_tokenized)))\n",
    "print(len(set(twice_tokenized)))\n",
    "print(len(set(blackpink_tokenized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From type count, we can calculate *lexical richness*: the number of unique words divided by the number of total words. Which Kpop video has the most lexically diverse comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(comments):\n",
    "    return len(set(comments))/len(comments)\n",
    "\n",
    "print(lexical_diversity(bts_tokenized))\n",
    "print(lexical_diversity(exo_tokenized))\n",
    "print(lexical_diversity(twice_tokenized))\n",
    "print(lexical_diversity(blackpink_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the frequency of specific types in the BTS dataset. We can see what the most common types are. We can also compare several band members to see who is mentioned more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency distribution of types.\n",
    "fdistbts = FreqDist(bts_tokenized)\n",
    "fdistexo = FreqDist(exo_tokenized)\n",
    "fdisttwice = FreqDist(twice_tokenized)\n",
    "fdistblackpink = FreqDist(blackpink_tokenized)\n",
    "\n",
    "# 50 most frequent types in the BTS video comments.\n",
    "fdistbts.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine specific words in the BTS dataset. We can compare several band members to see who is mentioned more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of a specific type.\n",
    "print(fdistbts[\"jimin\"])\n",
    "print(fdistbts[\"jungkook\"])\n",
    "print(fdistbts[\"suga\"])\n",
    "\n",
    "# Just for fun, let's see how often 'BTS' appears in the comments of rival group EXO's video, and vice versa.\n",
    "print(fdistexo[\"bts\"])\n",
    "print(fdistbts[\"exo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some more peculiar types: those that appear only once (hapax legomena), those that are extremely long, and those that are both long and frequently occurring. Such words often add a different perspective on a corpus of text (they're a bit like linguistic outliers!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many hapax legomena are there in the BTS comments?\n",
    "len(fdistbts.hapaxes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just look at the first 100.\n",
    "print(fdistbts.hapaxes()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the long words (more than 50 characters) in the BTS comments.\n",
    "bts_vocab = set(bts_tokenized)\n",
    "bts_long_words = [word for word in bts_vocab if len(word)>50]\n",
    "sorted(bts_long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are too many URLs in the 'long words list'. Let's remove them and try again.\n",
    "bts_vocab_nourls = []\n",
    "\n",
    "for word in bts_vocab:\n",
    "    if not word.startswith('http'):\n",
    "        if not word.startswith('www'):\n",
    "            bts_vocab_nourls.append(word)\n",
    "\n",
    "bts_long_words = [word for word in bts_vocab_nourls if len(word)>50]\n",
    "sorted(bts_long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's examine the words of at least 12 characters that occur more than 5 times.\n",
    "bts_long_frequent_words = [word for word in bts_vocab_nourls if len(word)>12 and fdistbts[word]>5]\n",
    "sorted(bts_long_frequent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step V: Part-of-speech (POS) tagging\n",
    "\n",
    "Having examined counts of specific types, let's take a step back and group the types into their POS categories. NLTK's default POS tagger uses tags from the Penn Treebank project: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save computing time, let's just look at the tags of the first 10,000 types in the BTS comments.\n",
    "bts_tagged = nltk.pos_tag(bts_tokenized[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an error when running the above code block, run this one. Otherwise, ignore it.\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rank the frequency of the different POS tags.\n",
    "bts_tag_fd = nltk.FreqDist(tag for (word, tag) in bts_tagged)\n",
    "print(bts_tag_fd.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's connect the frequency of the POS tags with the types, and print the ten most frequent types.\n",
    "bts_type_tag_fd = nltk.FreqDist(bts_tagged)\n",
    "bts_type_tag_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the types within a certain POS category by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most popular verbs (base form)? How accurate is the tagger?\n",
    "[typetag[0] for (typetag, _) in bts_type_tag_fd.most_common() if typetag[1] == \"VB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step VI: N-grams and collocations\n",
    "\n",
    "N-grams are words that co-occur within a given window: 2-grams (bigrams) are two words that co-occur, 3-grams (trigrams) are three words that co-occur, etc. The window is typically just one word (i.e., the words must be next to each other). \n",
    "\n",
    "N-gram collocations are n-grams that occur more often than we would expect based on the frequency of the individual words. We can compute them in Python using Pointwise Mutual Information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = BigramCollocationFinder.from_words(bts_tokenized)\n",
    "\n",
    "# Top 20 bigrams.\n",
    "print(bigram_finder.nbest(bigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter the results to only see the top 20 bigrams that appear at least five times.\n",
    "bigram_finder.apply_freq_filter(5)\n",
    "print(bigram_finder.nbest(bigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for trigrams.\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "trigram_finder = TrigramCollocationFinder.from_words(bts_tokenized)\n",
    "trigram_finder.apply_freq_filter(5)\n",
    "print(trigram_finder.nbest(trigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-Ended Exercises and Questions\n",
    "1. Rerun the above analyses for the BTS video for the EXO video (as they are rival bands!).\n",
    "2. What are the most common adjectives in the BTS and EXO comments? The most common proper nouns?\n",
    "3. What are the most common short words (length < 5)?\n",
    "4. How often do the words 'love' and 'hate' occur in the BTS and EXO video comments?\n",
    "5. What are the biggest issues with the above analyses, as applied to social web text vs. a traditional text corpus?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
